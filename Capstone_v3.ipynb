{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Capstone_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPo+s2RNUFKy5SCcRBGIv0s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1a8a509e265d42fd89c9231638354ab4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_39f3482f7050470a920743d91bc34ff3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_06863a39d16b4be6b41bd694b991b926",
              "IPY_MODEL_c5b194ae478e465c95e10c771a713405",
              "IPY_MODEL_2c626a8bb17745ce817025463f1a233f"
            ]
          }
        },
        "39f3482f7050470a920743d91bc34ff3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "06863a39d16b4be6b41bd694b991b926": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cfd4282e99244f36b1ae12f78cdd8085",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e1c5a1fbb7b344dfa9a2c6f76d8e9bfd"
          }
        },
        "c5b194ae478e465c95e10c771a713405": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a77cf3e5d58445b9b46ba3a10cdc0750",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 384,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 384,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_70cfdb034c4d4bd79a9b6d120de994c8"
          }
        },
        "2c626a8bb17745ce817025463f1a233f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_89236e204a2f45deaeef24fadfbcbedc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 384/384 [00:00&lt;00:00, 11.9kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_48e022d7179d4f3982af12b6298155b0"
          }
        },
        "cfd4282e99244f36b1ae12f78cdd8085": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e1c5a1fbb7b344dfa9a2c6f76d8e9bfd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a77cf3e5d58445b9b46ba3a10cdc0750": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "70cfdb034c4d4bd79a9b6d120de994c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "89236e204a2f45deaeef24fadfbcbedc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "48e022d7179d4f3982af12b6298155b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0a93d36309c8475a83477f38d6371da0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_dce79ef1255547aa84fa2ba9a4d18b02",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d83aa3493fa2471ba1da00f704d0f7e2",
              "IPY_MODEL_c5b631d07d8846f79ff864ab79c7e6af",
              "IPY_MODEL_6d29098c2d1f455b98ca4e7a05430554"
            ]
          }
        },
        "dce79ef1255547aa84fa2ba9a4d18b02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d83aa3493fa2471ba1da00f704d0f7e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e5e40417cf1542dda2b71091adbf02fa",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e33d4fdf795f4603918280db37bb5dc2"
          }
        },
        "c5b631d07d8846f79ff864ab79c7e6af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2d759ce3ba61475d9b1b896974577895",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f7f00851206e4eee9bd61cb325a5ecf5"
          }
        },
        "6d29098c2d1f455b98ca4e7a05430554": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c38c665d3fbe4e44a11333f2a0d0c871",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 904kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_aa231b5a15514082803b29d896593c6f"
          }
        },
        "e5e40417cf1542dda2b71091adbf02fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e33d4fdf795f4603918280db37bb5dc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2d759ce3ba61475d9b1b896974577895": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f7f00851206e4eee9bd61cb325a5ecf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c38c665d3fbe4e44a11333f2a0d0c871": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "aa231b5a15514082803b29d896593c6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d671575608de4322b6d5eeeb8fba8610": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_97f21e4099254c618092ca0890daa741",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c7234a659418402e876321c9edb92a58",
              "IPY_MODEL_0857b9e320584fd288a81b8844c44588",
              "IPY_MODEL_887461963dc547bbb60cb38c01e2ccfa"
            ]
          }
        },
        "97f21e4099254c618092ca0890daa741": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c7234a659418402e876321c9edb92a58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8d5ad8465ea5438fbc038b7ad6eefbe8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d58da45558594848a0294413820ef136"
          }
        },
        "0857b9e320584fd288a81b8844c44588": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d952d208f1544b209f5f6d0304c97aca",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 327051810,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 327051810,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b8921bf66fe948c98e84b8caff3d1b6c"
          }
        },
        "887461963dc547bbb60cb38c01e2ccfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ceb82a2fa5b04885bcf8c94f52be2282",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 327M/327M [00:18&lt;00:00, 23.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e5771528ef10478fa83ff83459057a64"
          }
        },
        "8d5ad8465ea5438fbc038b7ad6eefbe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d58da45558594848a0294413820ef136": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d952d208f1544b209f5f6d0304c97aca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b8921bf66fe948c98e84b8caff3d1b6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ceb82a2fa5b04885bcf8c94f52be2282": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e5771528ef10478fa83ff83459057a64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishnarevi/Capstone-Part-1/blob/main/Capstone_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ6oIhPQExDl"
      },
      "source": [
        "In this notebook, we show how we can take advantage of these recent advances to train a long form question answering system which takes in a question, fetches relevant passages from a document corpus, and writes a multi-sentence answer based on the question and retrieved passages.In particular, training embedding-based retrieval models to gather supporting evidence for open-domain questions is relatively new research area: the last few months have seen some significant progress in cases where direct supervision is available, or with extensive task-specific pretraining. Here, we show how our custom dataset allows us to train a dense retrieval system without access to either, making dense retrieval models more accessible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTqWP9ITEwE4"
      },
      "source": [
        "## 1.a - Preliminaries\n",
        "The implementation presented here relies on the Hugging Face ðŸ¤—transformers and ðŸ¤—nlp libraries. Wikipedia indexing relies on faiss for the dense version. You can get all of these by running:\n",
        "\n",
        "<!-- pip install elasticsearch -->\n",
        "pip install faiss_gpu\n",
        "pip install nlp\n",
        "pip install transformers\n",
        "<!-- \n",
        "wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.7.1-linux-x86_64.tar.gz\n",
        "tar -xzvf elasticsearch-7.7.1-linux-x86_64.tar.gz -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5u14eRYn09Kp",
        "outputId": "7988c80f-af22-44f1-8463-fa7e66d5aac3"
      },
      "source": [
        "!pip install faiss_gpu nlp transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss_gpu\n",
            "  Downloading faiss_gpu-1.7.1.post2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89.7 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89.7 MB 8.1 kB/s \n",
            "\u001b[?25hCollecting nlp\n",
            "  Downloading nlp-0.4.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.7 MB 66.6 MB/s \n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.8 MB 85.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nlp) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from nlp) (3.0.12)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from nlp) (1.1.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from nlp) (0.3.4)\n",
            "Requirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from nlp) (3.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from nlp) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from nlp) (4.62.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 243 kB 78.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp) (2021.5.30)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.0.16-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50 kB 8.8 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3 MB 66.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 895 kB 49.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 636 kB 47.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->nlp) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->nlp) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->nlp) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: xxhash, tokenizers, sacremoses, pyyaml, huggingface-hub, transformers, nlp, faiss-gpu\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed faiss-gpu-1.7.1.post2 huggingface-hub-0.0.16 nlp-0.4.0 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.0 xxhash-2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VgSm5-serJa"
      },
      "source": [
        "import functools\n",
        "import math\n",
        "import os  # noqa: F401\n",
        "from random import choice, randint\n",
        "from time import time\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.checkpoint as checkpoint\n",
        "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
        "from tqdm import tqdm\n",
        "\n",
        "import faiss  # noqa: F401\n",
        "import nlp  # noqa: F401\n",
        "import pandas as pd\n",
        "from transformers import AdamW, AutoModel, AutoModelForSeq2SeqLM, AutoTokenizer, get_linear_schedule_with_warmup\n",
        "import json"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ddm2PdNBlkvZ",
        "outputId": "93738601-cc82-4435-8e66-ac5cb4bd3538"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xS7XbaolnKg"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/TSAI/Capstone_1')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EgAlp0clpW5"
      },
      "source": [
        "# from lfqa_utils import *"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzpLMu5SeE5A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa5764e3-addb-4669-dde9-3603729a60fb"
      },
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "path = '/content/drive/MyDrive/TSAI/Capstone_1'"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oB1FubpEnOWG",
        "outputId": "f3ed4638-9f3f-4c9f-93e7-6307d71846c7"
      },
      "source": [
        "folder = \"retriever_models\"\n",
        "# os.chdir(path)\n",
        "print(\"current dir is: %s\" % (os.getcwd()))\n",
        "\n",
        "if os.path.isdir(folder):\n",
        "    print(\"Report directory exists\")\n",
        "else:\n",
        "    print(\"Report directory Doesn't exists, creating one\")\n",
        "    os.mkdir(folder)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current dir is: /content\n",
            "Report directory Doesn't exists, creating one\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8v6YJLUleqY"
      },
      "source": [
        "with open(path+'/train_data.json') as f:\n",
        "        train = json.load(f)\n",
        "with open(path+'/test_data.json') as f:\n",
        "        test = json.load(f)\n",
        "with open(path+'/final_data.json') as f:\n",
        "        passage_snippets = json.load(f)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrV7bVHQeyfg",
        "outputId": "6efdd7a5-1d21-4a29-f8ac-54ac27bbc649"
      },
      "source": [
        "train[0]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 1,\n",
              " 'x': 'What does delete_submodule do to the given submodule from self?',\n",
              " 'y': 'Deletes',\n",
              " 'z': 'This installs empty Modules where none exist yet if they are subpaths of target. target â€“ The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.) m â€“ The submodule itself; the actual object we want to install in the current Module  this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute) bool Return the Python code generated from the Graph underlying this GraphModule. Deletes all unused submodules from self. A Module is considered â€œusedâ€ if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target â€“ The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)  submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name> folder (Union[str, os.PathLike]) â€“ The folder to write the code out to module_name (str) â€“ Top-level name to use for the Module while writing out the code'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsKLzwpWfQNQ",
        "outputId": "4233a02f-d37f-4b27-98a0-1c3e074743bd"
      },
      "source": [
        "len(train)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25130"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVg6eAz9e6aP",
        "outputId": "c14faf45-a275-4c8d-8368-bcd3fcc65fe1"
      },
      "source": [
        "test[100]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 101,\n",
              " 'x': 'What kind of function does nn.ReLU6 apply?',\n",
              " 'y': 'element-wise',\n",
              " 'z': 'nn.ReLU6 Applies the element-wise function:'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZUuSCe1fSy_",
        "outputId": "e01fad50-820c-4424-e4af-53f60922dede"
      },
      "source": [
        "len(test)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6283"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dspIob65hT14"
      },
      "source": [
        "### Retrieving Support Documents with an ELI5-Trained Dense Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io61B76Fh7ro"
      },
      "source": [
        "The sparse retriever works by finding passages which feature the words from the query. However, it has no way to know a priori which of these words are more important in context, and seems to struggle with understanding the central theme of the query (human-perceived temperature).\n",
        "\n",
        "Thankfully, some recent works have taken advantage of advances in pre-trained contextual word representations to solve this problem. Models such as DPR or REALM for example learn to compute a vector representation of the query, as well as vector representations of Wikipedia passages in such a way that the passages that best answers a question maximize the dot product between the two representations. Retrieval is then reduced to a Maximum Inner Product Search, which can be executed efficiently using systems like FAISS.\n",
        "\n",
        "These successes are very encouraging for our Open-Domain Long Form QA application. However, our task and setup do not quite meet the requirements of either of either of these approaches. On the one hand, the DPR system is trained using gold passage annotations: most major QA dataset tell the system which Wikipedia passage contains the answer. Unfortunately, we do not have such annotations for the ELI5 data. On the other hand, while REALM is trained without passage supervision, it requires a pretty expensive pre-training step with an Inverse Cloze Task (100,000 steps with batch size 4096), and the ability to re-compute the embeddings of all Wikipedia passages regularly during training.\n",
        "\n",
        "In order to train a similar dense retrieval system at reduced cost without having access to gold passage annotation, we will have to take advantage of another unique feature of our dataset, namely the fact that the long form answers are quite similar in style to the Wikipedia passages we want to index. Our hypothesis then is that if we train a system to embed the questions and answers in our dataset in a way that allows us to easily match questions to answers, then using the answer embedder on Wikipedia passages should allow us to similarly match questions to supporting evidence from Wikipedia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PbDg4xciQn3"
      },
      "source": [
        "4.a - Contrastive Training with ELI5 In-Batch Negatives\n",
        "As mentioned above, we want to train a system to produce question and answer embeddings, such that the dot product between the representation of a question and any of its answers is greater than between it and answers of all of the other questions in the dataset.\n",
        "\n",
        "Unfortunately, actually comparing all questions to all answers before taking every single gradient step is computationally prohibitive: instead, we follow previous work in simply processing medium to large batches of question-answer pairs, and making sure that the dot product of a question with its answer is larger than with all other answers in the batch, and vice versa.\n",
        "\n",
        "We use a cross-entropy loss for the multinomial distribution over all of the answers (or questions) in a batch, and make use of PyTorch gradient checkpointing to be able to use large batches with limited GPU memory: you can find all implementation details in the RetrievalQAEmbedder class in eli5_utils.py.\n",
        "\n",
        "We use a single BERT-style pre-trained model to embed the questions and answers, and learn different projection matrices to bring both representations down to dimension 128: the projection matrices are trained from scratch as the sentence embedding model is fine-tuned. We found that the 8-layer distilled version of BERT from the Well-Read Students Learn Better paper performed as well or better as full BERT for a notable gain in computation speed: if you want an even faster model, that work provides pre-trained models spanning the full range of computation/accuracy trade-offs.\n",
        "\n",
        "The model can than be trained with the following code: with batch size 32/512 on a single 16GB GPU, you can run 10 training epochs in under 6 hours."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJa0ofK7f65J"
      },
      "source": [
        "###############\n",
        "# retriever training\n",
        "###############\n",
        "class ELI5DatasetQARetriver(Dataset):\n",
        "    def __init__(self, examples_array, num_rows, extra_answer_threshold=3, min_answer_length=2, training=True, n_samples=None):\n",
        "        self.data = examples_array\n",
        "        self.answer_thres = extra_answer_threshold\n",
        "        self.min_length = min_answer_length\n",
        "        self.training = training\n",
        "        self.n_samples = num_rows if n_samples is None else n_samples\n",
        "        self.num_rows = num_rows\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "    def make_example(self, idx):\n",
        "        example = self.data[idx]\n",
        "        question = example[\"x\"]\n",
        "        # if self.training:\n",
        "        #     answers = [a for i, (a, sc) in enumerate(zip(example[\"answers\"][\"text\"], example[\"answers\"][\"score\"]))]\n",
        "        #     answer_tab = choice(answers).split(\" \")\n",
        "        #     start_idx = randint(0, max(0, len(answer_tab) - self.min_length))\n",
        "        #     answer_span = \" \".join(answer_tab[start_idx:])\n",
        "        # else:\n",
        "            # answer_span = example[\"answers\"][\"text\"][0]\n",
        "        answer = example[\"y\"]\n",
        "        return (question, answer)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.make_example(idx % self.num_rows)\n",
        "\n",
        "\n",
        "class RetrievalQAEmbedder(torch.nn.Module):\n",
        "    def __init__(self, sent_encoder, dim):\n",
        "        super(RetrievalQAEmbedder, self).__init__()\n",
        "        self.sent_encoder = sent_encoder\n",
        "        self.output_dim = 128\n",
        "        self.project_q = torch.nn.Linear(dim, self.output_dim, bias=False)\n",
        "        self.project_a = torch.nn.Linear(dim, self.output_dim, bias=False)\n",
        "        self.ce_loss = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "\n",
        "    def embed_sentences_checkpointed(self, input_ids, attention_mask, checkpoint_batch_size=-1):\n",
        "        # reproduces BERT forward pass with checkpointing\n",
        "        if checkpoint_batch_size < 0 or input_ids.shape[0] < checkpoint_batch_size:\n",
        "            return self.sent_encoder(input_ids, attention_mask=attention_mask)[1]\n",
        "        else:\n",
        "            # prepare implicit variables\n",
        "            device = input_ids.device\n",
        "            input_shape = input_ids.size()\n",
        "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
        "            head_mask = [None] * self.sent_encoder.config.num_hidden_layers\n",
        "            extended_attention_mask: torch.Tensor = self.sent_encoder.get_extended_attention_mask(\n",
        "                attention_mask, input_shape, device\n",
        "            )\n",
        "\n",
        "            # define function for checkpointing\n",
        "            def partial_encode(*inputs):\n",
        "                encoder_outputs = self.sent_encoder.encoder(inputs[0], attention_mask=inputs[1], head_mask=head_mask,)\n",
        "                sequence_output = encoder_outputs[0]\n",
        "                pooled_output = self.sent_encoder.pooler(sequence_output)\n",
        "                return pooled_output\n",
        "\n",
        "            # run embedding layer on everything at once\n",
        "            embedding_output = self.sent_encoder.embeddings(\n",
        "                input_ids=input_ids, position_ids=None, token_type_ids=token_type_ids, inputs_embeds=None\n",
        "            )\n",
        "            # run encoding and pooling on one mini-batch at a time\n",
        "            pooled_output_list = []\n",
        "            for b in range(math.ceil(input_ids.shape[0] / checkpoint_batch_size)):\n",
        "                b_embedding_output = embedding_output[b * checkpoint_batch_size : (b + 1) * checkpoint_batch_size]\n",
        "                b_attention_mask = extended_attention_mask[b * checkpoint_batch_size : (b + 1) * checkpoint_batch_size]\n",
        "                pooled_output = checkpoint.checkpoint(partial_encode, b_embedding_output, b_attention_mask)\n",
        "                pooled_output_list.append(pooled_output)\n",
        "            return torch.cat(pooled_output_list, dim=0)\n",
        "\n",
        "    def embed_questions(self, q_ids, q_mask, checkpoint_batch_size=-1):\n",
        "        q_reps = self.embed_sentences_checkpointed(q_ids, q_mask, checkpoint_batch_size)\n",
        "        return self.project_q(q_reps)\n",
        "\n",
        "    def embed_answers(self, a_ids, a_mask, checkpoint_batch_size=-1):\n",
        "        a_reps = self.embed_sentences_checkpointed(a_ids, a_mask, checkpoint_batch_size)\n",
        "        return self.project_a(a_reps)\n",
        "\n",
        "    def forward(self, q_ids, q_mask, a_ids, a_mask, checkpoint_batch_size=-1):\n",
        "        device = q_ids.device\n",
        "        q_reps = self.embed_questions(q_ids, q_mask, checkpoint_batch_size)\n",
        "        a_reps = self.embed_answers(a_ids, a_mask, checkpoint_batch_size)\n",
        "        compare_scores = torch.mm(q_reps, a_reps.t())\n",
        "        loss_qa = self.ce_loss(compare_scores, torch.arange(compare_scores.shape[1]).to(device))\n",
        "        loss_aq = self.ce_loss(compare_scores.t(), torch.arange(compare_scores.shape[0]).to(device))\n",
        "        loss = (loss_qa + loss_aq) / 2\n",
        "        return loss\n",
        "\n",
        "\n",
        "def make_qa_retriever_model(model_name=\"google/bert_uncased_L-8_H-512_A-8\", from_file=None, device=\"cuda\"):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    bert_model = AutoModel.from_pretrained(model_name).to(device)\n",
        "    # run bert_model on a dummy batch to get output dimension\n",
        "    d_ids = torch.LongTensor(\n",
        "        [[bert_model.config.bos_token_id if bert_model.config.bos_token_id is not None else 1]]\n",
        "    ).to(device)\n",
        "    d_mask = torch.LongTensor([[1]]).to(device)\n",
        "    sent_dim = bert_model(d_ids, attention_mask=d_mask)[1].shape[-1]\n",
        "    qa_embedder = RetrievalQAEmbedder(bert_model, sent_dim).to(device)\n",
        "    if from_file is not None:\n",
        "        param_dict = torch.load(from_file)  # has model weights, optimizer, and scheduler states\n",
        "        qa_embedder.load_state_dict(param_dict[\"model\"])\n",
        "    return tokenizer, qa_embedder\n",
        "\n",
        "\n",
        "def make_qa_retriever_batch(qa_list, tokenizer, max_len=128, device=\"cuda\"):\n",
        "    q_ls = [q for q, a in qa_list]\n",
        "    a_ls = [a for q, a in qa_list]\n",
        " \n",
        "    q_toks = tokenizer.batch_encode_plus(q_ls, max_length=max_len, pad_to_max_length=True)\n",
        "    \n",
        "    q_ids, q_mask = (torch.LongTensor(q_toks[\"input_ids\"]).to(device),torch.LongTensor(q_toks[\"attention_mask\"]).to(device),)\n",
        "    # print(len(a_ls))\n",
        "\n",
        "    a_toks = tokenizer.batch_encode_plus(a_ls, max_length=max_len, pad_to_max_length=True)\n",
        "    # TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
        "    # print(a_toks)\n",
        "    a_ids, a_mask = (\n",
        "        torch.LongTensor(a_toks[\"input_ids\"]).to(device),\n",
        "        torch.LongTensor(a_toks[\"attention_mask\"]).to(device),\n",
        "    )\n",
        "\n",
        "    return (q_ids, q_mask, a_ids, a_mask)\n",
        "\n",
        "\n",
        "def train_qa_retriever_epoch(model, dataset, tokenizer, optimizer, scheduler, args, e=0):\n",
        "    model.train()\n",
        "    # make iterator\n",
        "    train_sampler = RandomSampler(dataset)\n",
        "    model_collate_fn = functools.partial(\n",
        "        make_qa_retriever_batch, tokenizer=tokenizer, max_len=args.max_length, device=\"cuda\"\n",
        "    )\n",
        "    \n",
        "    data_loader = DataLoader(dataset, batch_size=args.batch_size, sampler=train_sampler, collate_fn=model_collate_fn)\n",
        "    epoch_iterator = tqdm(data_loader, desc=\"Iteration\", disable=True)\n",
        "    # print(next(iter(data_loader)).shape)\n",
        "    # accumulate loss since last print\n",
        "    loc_steps = 0\n",
        "    loc_loss = 0.0\n",
        "    st_time = time()\n",
        "    for step, batch in enumerate(epoch_iterator):\n",
        "        # print(\"q_ids\",q_ids.shape)\n",
        "        # print(\" q_mask,\", q_mask.shape)\n",
        "        # print(\"A_id\", a_ids.shape)\n",
        "        q_ids, q_mask, a_ids, a_mask = batch\n",
        "        pre_loss = model(q_ids, q_mask, a_ids, a_mask, checkpoint_batch_size=args.checkpoint_batch_size)\n",
        "        loss = pre_loss.sum()\n",
        "        # optimizer\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        model.zero_grad()\n",
        "        # some printing within the epoch\n",
        "        loc_loss += loss.item()\n",
        "        loc_steps += 1\n",
        "        if step % args.print_freq == 0 or step == 1:\n",
        "            print(\n",
        "                \"{:2d} {:5d} of {:5d} \\t L: {:.3f} \\t -- {:.3f}\".format(\n",
        "                    e, step, len(dataset) // args.batch_size, loc_loss / loc_steps, time() - st_time,\n",
        "                )\n",
        "            )\n",
        "            loc_loss = 0\n",
        "            loc_steps = 0\n",
        "\n",
        "\n",
        "def train_qa_retriever_joint_epoch(model, dataset_list, tokenizer, optimizer, scheduler, args, e=0):\n",
        "    model.train()\n",
        "    model_collate_fn = functools.partial(\n",
        "        make_qa_retriever_batch, tokenizer=tokenizer, max_len=args.max_length, device=\"cuda\"\n",
        "    )\n",
        "    # make iterator\n",
        "    train_samplers = [RandomSampler(dataset) for dataset in dataset_list]\n",
        "    data_loaders = [\n",
        "        DataLoader(dataset, batch_size=args.batch_size, sampler=train_sampler, collate_fn=model_collate_fn)\n",
        "        for dataset, train_sampler in zip(dataset_list, train_samplers)\n",
        "    ]\n",
        "    iterators = [iter(dloader) for dloader in data_loaders]\n",
        "    joint_iter = zip(*iterators)\n",
        "    # accumulate loss since last print\n",
        "    loc_steps = 0\n",
        "    loc_loss = 0.0\n",
        "    st_time = time()\n",
        "    for step, (batches,) in enumerate(zip(joint_iter)):\n",
        "        for batch in batches:\n",
        "            q_ids, q_mask, a_ids, a_mask = batch\n",
        "\n",
        "            loss = model(q_ids, q_mask, a_ids, a_mask, checkpoint_batch_size=args.checkpoint_batch_size)\n",
        "            # optimizer\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            model.zero_grad()\n",
        "            # some printing within the epoch\n",
        "            loc_loss += loss.item()\n",
        "            loc_steps += 1\n",
        "        if step % args.print_freq == 0:\n",
        "            print(\n",
        "                \"{:2d} {:5d} of {:5d} \\t L: {:.3f} \\t -- {:.3f}\".format(\n",
        "                    e, step, len(dataset_list[0]) // args.batch_size, loc_loss / loc_steps, time() - st_time,\n",
        "                )\n",
        "            )\n",
        "            loc_loss = 0\n",
        "            loc_steps = 0\n",
        "\n",
        "\n",
        "def evaluate_qa_retriever(model, dataset, tokenizer, args):\n",
        "    model.eval()\n",
        "    # make iterator\n",
        "    eval_sampler = SequentialSampler(dataset)\n",
        "    model_collate_fn = functools.partial(\n",
        "        make_qa_retriever_batch, tokenizer=tokenizer, max_len=args.max_length, device=\"cuda\"\n",
        "    )\n",
        "    data_loader = DataLoader(dataset, batch_size=args.batch_size, sampler=eval_sampler, collate_fn=model_collate_fn)\n",
        "    epoch_iterator = tqdm(data_loader, desc=\"Iteration\", disable=True)\n",
        "    tot_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "            q_ids, q_mask, a_ids, a_mask = batch\n",
        "            loss = model(q_ids, q_mask, a_ids, a_mask)\n",
        "            tot_loss += loss.item()\n",
        "        return tot_loss / (step + 1)\n",
        "\n",
        "\n",
        "def train_qa_retriever(qar_model, qar_tokenizer, qar_train_dset, qar_valid_dset, qar_args):\n",
        "    qar_optimizer = AdamW(qar_model.parameters(), lr=qar_args.learning_rate, eps=1e-8)\n",
        "    qar_scheduler = get_linear_schedule_with_warmup(\n",
        "        qar_optimizer,\n",
        "        num_warmup_steps=100,\n",
        "        num_training_steps=(qar_args.num_epochs + 1) * math.ceil(len(qar_train_dset) / qar_args.batch_size),\n",
        "    )\n",
        "    for e in range(qar_args.num_epochs):\n",
        "        train_qa_retriever_epoch(qar_model, qar_train_dset, qar_tokenizer, qar_optimizer, qar_scheduler, qar_args, e)\n",
        "        m_save_dict = {\n",
        "            \"model\": qar_model.state_dict(),\n",
        "            \"optimizer\": qar_optimizer.state_dict(),\n",
        "            \"scheduler\": qar_scheduler.state_dict(),\n",
        "        }\n",
        "        print(\"Saving model {}\".format(qar_args.model_save_name))\n",
        "        torch.save(m_save_dict, \"{}_{}.pth\".format(qar_args.model_save_name, e))\n",
        "        eval_loss = evaluate_qa_retriever(qar_model, qar_valid_dset, qar_tokenizer, qar_args)\n",
        "        print(\"Evaluation loss epoch {:4d}: {:.3f}\".format(e, eval_loss))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301,
          "referenced_widgets": [
            "1a8a509e265d42fd89c9231638354ab4",
            "39f3482f7050470a920743d91bc34ff3",
            "06863a39d16b4be6b41bd694b991b926",
            "c5b194ae478e465c95e10c771a713405",
            "2c626a8bb17745ce817025463f1a233f",
            "cfd4282e99244f36b1ae12f78cdd8085",
            "e1c5a1fbb7b344dfa9a2c6f76d8e9bfd",
            "a77cf3e5d58445b9b46ba3a10cdc0750",
            "70cfdb034c4d4bd79a9b6d120de994c8",
            "89236e204a2f45deaeef24fadfbcbedc",
            "48e022d7179d4f3982af12b6298155b0",
            "0a93d36309c8475a83477f38d6371da0",
            "dce79ef1255547aa84fa2ba9a4d18b02",
            "d83aa3493fa2471ba1da00f704d0f7e2",
            "c5b631d07d8846f79ff864ab79c7e6af",
            "6d29098c2d1f455b98ca4e7a05430554",
            "e5e40417cf1542dda2b71091adbf02fa",
            "e33d4fdf795f4603918280db37bb5dc2",
            "2d759ce3ba61475d9b1b896974577895",
            "f7f00851206e4eee9bd61cb325a5ecf5",
            "c38c665d3fbe4e44a11333f2a0d0c871",
            "aa231b5a15514082803b29d896593c6f",
            "d671575608de4322b6d5eeeb8fba8610",
            "97f21e4099254c618092ca0890daa741",
            "c7234a659418402e876321c9edb92a58",
            "0857b9e320584fd288a81b8844c44588",
            "887461963dc547bbb60cb38c01e2ccfa",
            "8d5ad8465ea5438fbc038b7ad6eefbe8",
            "d58da45558594848a0294413820ef136",
            "d952d208f1544b209f5f6d0304c97aca",
            "b8921bf66fe948c98e84b8caff3d1b6c",
            "ceb82a2fa5b04885bcf8c94f52be2282",
            "e5771528ef10478fa83ff83459057a64"
          ]
        },
        "id": "O30WJrHHhVcS",
        "outputId": "a0c69405-4e4f-4b0a-807e-437a19e4e5b6"
      },
      "source": [
        "# training arguments\n",
        "class ArgumentsQAR():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 512\n",
        "        self.max_length = 128\n",
        "        self.checkpoint_batch_size = 32\n",
        "        self.print_freq = 100\n",
        "        self.pretrained_model_name = \"google/bert_uncased_L-8_H-768_A-12\"\n",
        "        self.model_save_name = \"/content/retriever_models/eli5_retriever_model_l-8_h-768_b-512-512\"\n",
        "        self.learning_rate = 2e-4\n",
        "        self.num_epochs = 1\n",
        "\n",
        "qar_args = ArgumentsQAR()\n",
        "\n",
        "# prepare torch Dataset objects\n",
        "qar_train_dset = ELI5DatasetQARetriver(train,num_rows=len(train), training=True)\n",
        "qar_valid_dset = ELI5DatasetQARetriver(test,num_rows=len(test), training=False)\n",
        "\n",
        "# load pre-trained BERT and make model\n",
        "qar_tokenizer, qar_model = make_qa_retriever_model(\n",
        "        model_name=qar_args.pretrained_model_name,\n",
        "        from_file=None,\n",
        "        device=\"cuda\"\n",
        ")\n",
        "\n",
        "# train the model\n",
        "train_qa_retriever(qar_model, qar_tokenizer, qar_train_dset, qar_valid_dset, qar_args)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a8a509e265d42fd89c9231638354ab4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/384 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a93d36309c8475a83477f38d6371da0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d671575608de4322b6d5eeeb8fba8610",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/327M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google/bert_uncased_L-8_H-768_A-12 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 0     0 of    49 \t L: 6.507 \t -- 17.235\n",
            " 0     1 of    49 \t L: 6.552 \t -- 34.857\n",
            "Saving model /content/retriever_models/eli5_retriever_model_l-8_h-768_b-512-512\n",
            "Evaluation loss epoch    0: 5.374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGh7wbrQunpO"
      },
      "source": [
        "Once the model is trained, it can be used to compute passage embeddings for all document corpus. The make_qa_dense_index method takes advantage of numpy memory-mapping, so embeddings are written directly to disk. Again with a single GPU, computing the full set of passage embeddings should take about 18 hours."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9a5qIB0USVm",
        "outputId": "fabc9b1e-6f82-47a3-aa2e-f2d36092e144"
      },
      "source": [
        "type(passage_snippets[0 * 512 : (0 + 1) * 512][0]['z'])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ7knv0wTyA1"
      },
      "source": [
        "# passages = [p for p in passage_snippets[0 * 512 : (0 + 1) * 512][\"z\"]]\n",
        "# passages"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yK4na7_YUEW"
      },
      "source": [
        "def embed_passages_for_retrieval(passages, tokenizer, qa_embedder, max_length=128, device=\"cuda\"):\n",
        "    a_toks = tokenizer.batch_encode_plus(passages, max_length=max_length, pad_to_max_length=True)\n",
        "    a_ids, a_mask = (\n",
        "        torch.LongTensor(a_toks[\"input_ids\"]).to(device),\n",
        "        torch.LongTensor(a_toks[\"attention_mask\"]).to(device),\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        a_reps = qa_embedder.embed_answers(a_ids, a_mask).cpu().type(torch.float)\n",
        "    return a_reps.numpy()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2WtrIwdZsoH"
      },
      "source": [
        "\n",
        "def embed_questions_for_retrieval(q_ls, tokenizer, qa_embedder, device=\"cuda\"):\n",
        "    q_toks = tokenizer.batch_encode_plus(q_ls, max_length=128, pad_to_max_length=True)\n",
        "    q_ids, q_mask = (\n",
        "        torch.LongTensor(q_toks[\"input_ids\"]).to(device),\n",
        "        torch.LongTensor(q_toks[\"attention_mask\"]).to(device),\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        q_reps = qa_embedder.embed_questions(q_ids, q_mask).cpu().type(torch.float)\n",
        "    return q_reps.numpy()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFDZGQaNvpGn"
      },
      "source": [
        "def make_qa_dense_index(\n",
        "    qa_embedder,\n",
        "    tokenizer,\n",
        "    passages_dset,\n",
        "    batch_size=512,\n",
        "    max_length=128,\n",
        "    index_name=\"kilt_passages_reps.dat\",\n",
        "    dtype=\"float32\",\n",
        "    device=\"cuda\",\n",
        "):\n",
        "    st_time = time()\n",
        "    fp = np.memmap(index_name, dtype=dtype, mode=\"w+\", shape=(len(passages_dset),128))\n",
        "    n_batches = math.ceil(len(passages_dset) / batch_size)\n",
        "    for i in range(n_batches):\n",
        "        passages = [p[\"z\"] for p in passages_dset[i * batch_size : (i + 1) * batch_size]]\n",
        "        reps = embed_passages_for_retrieval(passages, tokenizer, qa_embedder, max_length, device)\n",
        "        fp[i * batch_size : (i + 1) * batch_size] = reps\n",
        "        if i % 50 == 0:\n",
        "            print(i, time() - st_time)\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fB4Dp5hukVm",
        "outputId": "0263e3f8-c705-4c6b-e20f-c1acab9077ca"
      },
      "source": [
        "if not os.path.isfile('wiki40b_passages_reps_32_l-8_h-768_b-512-512.dat'):\n",
        "\n",
        "  make_qa_dense_index(\n",
        "          qar_model, qar_tokenizer, passage_snippets, device='cuda',\n",
        "          index_name='wiki40b_passages_reps_32_l-8_h-768_b-512-512.dat' )"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 3.407412528991699\n",
            "50 172.72707676887512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01GnGvUo1YPC"
      },
      "source": [
        "### 4.b - Using the Trained Dense Retriever and Wikipedia Index\n",
        "Now that we have trained our model to compute query and answer embeddings and used it to compute passage embeddings for all our Wikipedia snippets, let's see whether it can actually find supporting evidence for a new question. Recall the the two steps to using the dense retriever: we first compute an embedding for a new question, then do Max Inner Product Search with the pre-computed passage representations.\n",
        "\n",
        "The MIPS part can be executed efficiently with the faiss library. Additionally, since we computed 128-dimensional passage embeddings, the whole of the representations fits on a GPU, making retrieval even faster. We can create the faiss_gpu index with the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzWm_WCC1g--"
      },
      "source": [
        "faiss_res = faiss.StandardGpuResources()\n",
        "wiki40b_passage_reps = np.memmap(\n",
        "            'wiki40b_passages_reps_32_l-8_h-768_b-512-512.dat',\n",
        "            dtype='float32', mode='r',\n",
        "            # shape=(wiki40b_snippets.num_rows, 128)\n",
        "            # wiki40b_snippets.num_rows = 11378343,english sections from wiki40B dataset\n",
        "            shape=(len(passage_snippets), 128)\n",
        ")\n",
        "\n",
        "wiki40b_index_flat = faiss.IndexFlatIP(128)\n",
        "wiki40b_gpu_index = faiss.index_cpu_to_gpu(faiss_res, 0, wiki40b_index_flat)\n",
        "wiki40b_gpu_index.add(wiki40b_passage_reps)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HFtgUv-30K4"
      },
      "source": [
        "\n",
        "# build a support document for the question out of Wikipedia snippets\n",
        "# def query_qa_dense_index(\n",
        "#     question, qa_embedder, tokenizer, wiki_passages, wiki_index, n_results=10, min_length=2, device=\"cuda\"\n",
        "# ):\n",
        "#     q_rep = embed_questions_for_retrieval([question], tokenizer, qa_embedder, device=device)\n",
        "#     D, I = wiki_index.search(q_rep, 2 * n_results)\n",
        "#     res_passages = [wiki_passages[int(i)] for i in I[0]]\n",
        "#     support_doc = \"<P> \" + \" <P> \".join([p[\"z\"] for p in res_passages])\n",
        "#     res_list = [p['z'] for p in res_passages]\n",
        "\n",
        "#     for r, sc in zip(res_list, D[0]):\n",
        "#         r[\"score\"] = float(sc)\n",
        "#     return support_doc, res_list\n",
        "# find nearest neighbors of an answer or declarative text in Wikipedia snippets\n",
        "\n",
        "# build a support document for the question out of Wikipedia snippets\n",
        "def query_qa_dense_index(\n",
        "    question, qa_embedder, tokenizer, wiki_passages, wiki_index, n_results=10, min_length=2, device=\"cuda\"\n",
        "):\n",
        "    q_rep = embed_questions_for_retrieval([question], tokenizer, qa_embedder, device=device)\n",
        "    D, I = wiki_index.search(q_rep, 2 * n_results)\n",
        "    res_passages = [wiki_passages[int(i)] for i in I[0]]\n",
        "    support_doc = \"<P> \" + \" <P> \".join([p[\"z\"] for p in res_passages])\n",
        "    res_list = [dict([(k, p[k]) for k in [\"id\",\"z\"]]) for p in res_passages]\n",
        "    res_list = [res for res in res_list if len(res[\"z\"].split()) > min_length][:n_results]\n",
        "    for r, sc in zip(res_list, D[0]):\n",
        "        r[\"score\"] = float(sc)\n",
        "    return support_doc, res_list"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1hJ2_U12lYQ"
      },
      "source": [
        "Now we can use the query_qa_dense_index function to query the dense index for our running example question :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fhJqmCP52wnA",
        "outputId": "ffbb9bb1-c173-4df0-952b-e49a5e7d26b1"
      },
      "source": [
        "question = test[6]['x']\n",
        "question"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Bubble up non null rows of a 3-D Tensor PyTorch'"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nYoEC2yU2jUA",
        "outputId": "e180edb6-e628-4c71-a210-1f018a680b2f"
      },
      "source": [
        "doc, res_list = query_qa_dense_index(question, qar_model, qar_tokenizer, passage_snippets, wiki40b_gpu_index, device='cuda')\n",
        "print(res_list)\n",
        "df = pd.DataFrame({\n",
        "    \n",
        "    'Text': ['--- ' + question] + [res['z'] for res in res_list],\n",
        "})\n",
        "df.style.set_properties(**{'text-align': 'left'})"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'id': 6123, 'z': '<p>Separate the image channels into r,g,b using OpenCV,\\nThen use numpy mean and std function to calculate the mean and standard deviations for each channel.</p>\\n<p>Example for separating the image into rgb channels.</p>\\n<pre><code>import cv2\\nimport numpy as np\\n\\nimg = cv2.imread(&quot;image.jpg&quot;)\\nb = img[:,:,0]\\ng = img[:,:,1]\\nr = img[:,:,2]\\n</code></pre>\\n', 'score': 6.212342739105225}, {'id': 6037, 'z': \"<p>Assuming your output will be float, you can map that to str and apply join as a comma separated string, finally append row to csv.</p>\\n<pre><code>import pandas as pd\\nimport numpy as np\\nimport torch\\n\\noutput = torch.rand(5,5,5,5)\\npreds = output.sum(dim=[1,2,3])\\n\\nprint(preds)\\n\\nwith open('preds.csv','a') as fd:\\n    fd.write( ','.join(map(str, preds.detach().tolist())) + '\\\\n')\\n</code></pre>\\n\", 'score': 6.174643516540527}, {'id': 4002, 'z': \"<p>You can use <code>wandb.log()</code> with matplotlib. Create your plot using matplotlib:</p>\\n<pre><code>import matplotlib.pyplot as plt\\nimport numpy as np\\nx = np.linspace(0, 50)\\nfor i in range(1, 4):\\n    fig, ax = plt.subplots()\\n    y = x ** i\\n    ax.plot(x, y)\\n    wandb.log({'chart': ax})\\n</code></pre>\\n<p>Then when you look on your wandb dashboard for the run, you will see the plot rendered as plotly plot. Click the gear in the upper left hand corner to see a slider that lets you slide over training steps and see the plot at each step.</p>\\n\", 'score': 6.155918121337891}, {'id': 3571, 'z': \"<h3>Modifying your code to sort list of tuples</h3>\\n<p>In your code, instead of appending the cluster_int, just append the tuple (n, cluster_int), and then later while sorting, use lambda to sort by the second value of each tuple.</p>\\n<pre><code>for n in range(0,9):\\n    cluster_ = data2[data2['cluster_id']== n]\\n\\n    cluster_list = cluster_['y'].tolist()\\n    cluster_avg = sum(cluster_list)/len(cluster_list)\\n\\n    cluster_int = int(cluster_avg)\\n\\n    print(&quot;cluster_id : %d&quot; %n ,&quot;average : %d&quot; %cluster_int)\\n    lst.append((n,cluster_int))                              #&lt;-------\\n    a = sorted(lst, key = lambda x:x[1])                     #&lt;-------\\n\\n\\nprint(a)                                                     #&lt;-------\\n\\nordered_average = [average for cluster, average in a]        #&lt;-------\\nordered_clusters = [cluster for cluster, average in a]       #&lt;-------\\n\\nprint(ordered_average)                                       #&lt;-------\\nprint(ordered_clusters)                                      #&lt;-------\\n</code></pre>\\n<pre><code>#cluster and average together\\n[(4,112),(8,121,(1,127),(6,139),(5,149)]\\n\\n#averages sorted\\n[112, 121, 127, 139, 149]\\n\\n#clusters sorted\\n[4,8,1,6,5]\\n</code></pre>\\n<hr />\\n<h3>Alternate way using pandas</h3>\\n<p>A quicker way to do this would be directly sorting the pandas dataframe after groupby.</p>\\n<pre><code>print(df.groupby('cluster')['y'].mean().reset_index().sort_values('y'))\\n</code></pre>\\n\", 'score': 6.145504474639893}, {'id': 3045, 'z': '<p>You can simply return the encoded output in the <code>forward</code> function as follows:</p>\\n\\n<pre><code>class Autoencoder(nn.Module):\\n...\\n    def forward(self, x):\\n        x = self.encoder(x)\\n        encoded_x = x\\n        x = self.decoder(x)\\n        return x, encoded_x\\n</code></pre>\\n\\n<p>Modify the training function a little bit:</p>\\n\\n<pre><code>output, encoded_output = model(img)\\n</code></pre>\\n\\n<p>OR you can simply call <code>encoder</code>:</p>\\n\\n<pre><code>encoded_output = model.encoder(img)\\n</code></pre>\\n', 'score': 6.12684965133667}, {'id': 4703, 'z': \"<p>To calculate mean and std for the whole tensor you set no arguments</p>\\n\\n<pre><code>m = t.mean(); print(m) # if you don't set the dim for the whole tensor\\ns = t.std(); print(s) # if you don't set the dim for the whole tensor\\n</code></pre>\\n\\n<p>Then if your shape is 2,2,2 for instance, create tensors for broadcasting subtract and division.</p>\\n\\n<pre><code>ss = torch.empty(2,2,2).fill_(s)\\nprint(ss)\\n\\nmm = torch.empty(2,2,2).fill_(m)\\nprint(mm)\\n</code></pre>\\n\\n<p>At the moment <code>keepdim</code> is not working as expected when you don't set the <code>dim</code>.</p>\\n\\n<pre><code>m = t.mean(); print(m) # for the whole tensor\\ns = t.std(); print(s) # for the whole tensor\\n\\nm = t.mean(dim=0); print(m) # 0 means columns mean\\ns = t.std(dim=0); print(s) # 0 means columns mean\\n\\nm = t.mean(dim=1); print(m) # 1 means rows mean\\ns = t.std(dim=1); print(s) # 1 means rows mean\\n\\ns = t.mean(keepdim=True);print(s) # will not work\\nm = t.std(keepdim=True);print(m) # will not work\\n</code></pre>\\n\\n<p>If you set a dim as a tuple, then it will return mean for axes, you asked not for the whole.</p>\\n\", 'score': 6.1182451248168945}, {'id': 709, 'z': \"<p>You can implement your model as below:</p>\\n<pre><code>class Model(nn.Module):\\n       def __init__(self, conv_layers, classifier):\\n           super().__init__()\\n           self.conv_layers = conv_layers\\n           self.classifier = classifier\\n\\n       def forward(self,x):\\n           x = self.conv_layers(x)\\n           return self.classifier(x)\\n</code></pre>\\n<p>When declaring optimizer, only pass the parameters that you want to be updated.</p>\\n<pre><code>       model = Model(conv_layers, classifier)            \\n       optimizer = torch.optim.Adam(model.classifier.parameters(), lr=lr)\\n  \\n</code></pre>\\n<p>Now when you will do</p>\\n<pre><code>       loss.backward()\\n       optimizer.step()\\n       model.zero_grad()\\n</code></pre>\\n<p>only classifier params will be updated.</p>\\n<p>EDIT: After OP's comment, I am adding below for more generic use cases.</p>\\n<p>A more generic scenario</p>\\n<pre><code>   class Model(nn.Module):\\n       def __init__(self, modules):\\n           super().__init__()\\n           # supposing you have multiple modules declared like below. \\n           # You can also keep them as an array or dict too. \\n           # For this see nn.ModuleList or nn.ModuleDict in pytorch \\n           self.module0 = modules[0]\\n           self.module1 = modules[1]\\n           #..... and so on\\n\\n       def forward(self,x):\\n           # implement forward \\n\\n   # model and optimizer declarations\\n   model = Model(modules)  \\n   # assuming we want to update module0 and module1          \\n   optimizer = torch.optim.Adam([\\n       model.module0.parameters(), \\n       model.module1.parameters()\\n   ], lr=lr)\\n   # you can also provide different learning rate for different modules. \\n   # See [documentation][https://pytorch.org/docs/stable/optim.html]\\n   \\n   # when training  \\n   loss.backward()\\n   optimizer.step()\\n   model.zero_grad()\\n   # use model.zero_grad to remove gradient computed for all the modules.\\n   # optimizer.zero_grad only removes gradient for parameters that were passed to it. \\n</code></pre>\\n\", 'score': 6.109078884124756}, {'id': 394, 'z': '<p>Roughly, you need to define a criterion which is the Loss function, for example.</p>\\n<pre><code> import torch.nn as nn\\n criterion = nn.CrossEntropyLoss()\\n</code></pre>\\n<p>Then you need to divide each data entry into two, the image and the corresponding label. Feed the images to your CNN, evaluate with criterion (forward pass) and finally update the CNN (back-propagation).</p>\\n<pre><code># Forward pass\\noutputs = CNN(images)\\nloss = criterion(outputs, labels)\\n\\n# Back pass\\nloss.backward()\\n</code></pre>\\n<p>I suggest you to read some code examples of CNN from GitHub, perhaps a Python Notebook is the best option.</p>\\n', 'score': 6.10837984085083}, {'id': 3428, 'z': '<p>You can set excluded elements to zero: </p>\\n\\n<pre><code>A[range(A.shape[0]),Idx] = 0\\n</code></pre>\\n\\n<p>and sum tensor along rows:</p>\\n\\n<pre><code>b = A.sum(dim = 1,keepdim = True ) # b = torch.tensor([[5],  [9]])\\n</code></pre>\\n', 'score': 6.083930969238281}, {'id': 6388, 'z': \"<p>You can use simple einsum:</p>\\n<pre><code>#this gives you 2-D array (M,M)\\nnp.einsum('i,ijk-&gt;jk',a,b)\\n</code></pre>\\n<p>output:</p>\\n<pre><code>[[38 44]\\n [50 56]]\\n</code></pre>\\n<p>or another solution:</p>\\n<pre><code>#this gives you 3-D array (1,M,M)\\na[None,:]@b.swapaxes(0,1)\\n</code></pre>\\n<p>output:</p>\\n<pre><code>[[[38 44]\\n  [50 56]]]\\n</code></pre>\\n\", 'score': 6.082837104797363}]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "#T_fa864212_0faf_11ec_91bc_0242ac1c0002row0_col0,#T_fa864212_0faf_11ec_91bc_0242ac1c0002row1_col0,#T_fa864212_0faf_11ec_91bc_0242ac1c0002row2_col0,#T_fa864212_0faf_11ec_91bc_0242ac1c0002row3_col0,#T_fa864212_0faf_11ec_91bc_0242ac1c0002row4_col0,#T_fa864212_0faf_11ec_91bc_0242ac1c0002row5_col0,#T_fa864212_0faf_11ec_91bc_0242ac1c0002row6_col0,#T_fa864212_0faf_11ec_91bc_0242ac1c0002row7_col0,#T_fa864212_0faf_11ec_91bc_0242ac1c0002row8_col0,#T_fa864212_0faf_11ec_91bc_0242ac1c0002row9_col0,#T_fa864212_0faf_11ec_91bc_0242ac1c0002row10_col0{\n",
              "            text-align:  left;\n",
              "        }</style><table id=\"T_fa864212_0faf_11ec_91bc_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Text</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_fa864212_0faf_11ec_91bc_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "                        <td id=\"T_fa864212_0faf_11ec_91bc_0242ac1c0002row0_col0\" class=\"data row0 col0\" >--- Bubble up non null rows of a 3-D Tensor PyTorch</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_fa864212_0faf_11ec_91bc_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "                        <td id=\"T_fa864212_0faf_11ec_91bc_0242ac1c0002row1_col0\" class=\"data row1 col0\" ><p>Separate the image channels into r,g,b using OpenCV,\n",
              "Then use numpy mean and std function to calculate the mean and standard deviations for each channel.</p>\n",
              "<p>Example for separating the image into rgb channels.</p>\n",
              "<pre><code>import cv2\n",
              "import numpy as np\n",
              "\n",
              "img = cv2.imread(&quot;image.jpg&quot;)\n",
              "b = img[:,:,0]\n",
              "g = img[:,:,1]\n",
              "r = img[:,:,2]\n",
              "</code></pre>\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_fa864212_0faf_11ec_91bc_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "                        <td id=\"T_fa864212_0faf_11ec_91bc_0242ac1c0002row2_col0\" class=\"data row2 col0\" ><p>Assuming your output will be float, you can map that to str and apply join as a comma separated string, finally append row to csv.</p>\n",
              "<pre><code>import pandas as pd\n",
              "import numpy as np\n",
              "import torch\n",
              "\n",
              "output = torch.rand(5,5,5,5)\n",
              "preds = output.sum(dim=[1,2,3])\n",
              "\n",
              "print(preds)\n",
              "\n",
              "with open('preds.csv','a') as fd:\n",
              "    fd.write( ','.join(map(str, preds.detach().tolist())) + '\\n')\n",
              "</code></pre>\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_fa864212_0faf_11ec_91bc_0242ac1c0002level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "                        <td id=\"T_fa864212_0faf_11ec_91bc_0242ac1c0002row3_col0\" class=\"data row3 col0\" ><p>You can use <code>wandb.log()</code> with matplotlib. Create your plot using matplotlib:</p>\n",
              "<pre><code>import matplotlib.pyplot as plt\n",
              "import numpy as np\n",
              "x = np.linspace(0, 50)\n",
              "for i in range(1, 4):\n",
              "    fig, ax = plt.subplots()\n",
              "    y = x ** i\n",
              "    ax.plot(x, y)\n",
              "    wandb.log({'chart': ax})\n",
              "</code></pre>\n",
              "<p>Then when you look on your wandb dashboard for the run, you will see the plot rendered as plotly plot. Click the gear in the upper left hand corner to see a slider that lets you slide over training steps and see the plot at each step.</p>\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_fa864212_0faf_11ec_91bc_0242ac1c0002level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "                        <td id=\"T_fa864212_0faf_11ec_91bc_0242ac1c0002row4_col0\" class=\"data row4 col0\" ><h3>Modifying your code to sort list of tuples</h3>\n",
              "<p>In your code, instead of appending the cluster_int, just append the tuple (n, cluster_int), and then later while sorting, use lambda to sort by the second value of each tuple.</p>\n",
              "<pre><code>for n in range(0,9):\n",
              "    cluster_ = data2[data2['cluster_id']== n]\n",
              "\n",
              "    cluster_list = cluster_['y'].tolist()\n",
              "    cluster_avg = sum(cluster_list)/len(cluster_list)\n",
              "\n",
              "    cluster_int = int(cluster_avg)\n",
              "\n",
              "    print(&quot;cluster_id : %d&quot; %n ,&quot;average : %d&quot; %cluster_int)\n",
              "    lst.append((n,cluster_int))                              #&lt;-------\n",
              "    a = sorted(lst, key = lambda x:x[1])                     #&lt;-------\n",
              "\n",
              "\n",
              "print(a)                                                     #&lt;-------\n",
              "\n",
              "ordered_average = [average for cluster, average in a]        #&lt;-------\n",
              "ordered_clusters = [cluster for cluster, average in a]       #&lt;-------\n",
              "\n",
              "print(ordered_average)                                       #&lt;-------\n",
              "print(ordered_clusters)                                      #&lt;-------\n",
              "</code></pre>\n",
              "<pre><code>#cluster and average together\n",
              "[(4,112),(8,121,(1,127),(6,139),(5,149)]\n",
              "\n",
              "#averages sorted\n",
              "[112, 121, 127, 139, 149]\n",
              "\n",
              "#clusters sorted\n",
              "[4,8,1,6,5]\n",
              "</code></pre>\n",
              "<hr />\n",
              "<h3>Alternate way using pandas</h3>\n",
              "<p>A quicker way to do this would be directly sorting the pandas dataframe after groupby.</p>\n",
              "<pre><code>print(df.groupby('cluster')['y'].mean().reset_index().sort_values('y'))\n",
              "</code></pre>\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_fa864212_0faf_11ec_91bc_0242ac1c0002level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "                        <td id=\"T_fa864212_0faf_11ec_91bc_0242ac1c0002row5_col0\" class=\"data row5 col0\" ><p>You can simply return the encoded output in the <code>forward</code> function as follows:</p>\n",
              "\n",
              "<pre><code>class Autoencoder(nn.Module):\n",
              "...\n",
              "    def forward(self, x):\n",
              "        x = self.encoder(x)\n",
              "        encoded_x = x\n",
              "        x = self.decoder(x)\n",
              "        return x, encoded_x\n",
              "</code></pre>\n",
              "\n",
              "<p>Modify the training function a little bit:</p>\n",
              "\n",
              "<pre><code>output, encoded_output = model(img)\n",
              "</code></pre>\n",
              "\n",
              "<p>OR you can simply call <code>encoder</code>:</p>\n",
              "\n",
              "<pre><code>encoded_output = model.encoder(img)\n",
              "</code></pre>\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_fa864212_0faf_11ec_91bc_0242ac1c0002level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
              "                        <td id=\"T_fa864212_0faf_11ec_91bc_0242ac1c0002row6_col0\" class=\"data row6 col0\" ><p>To calculate mean and std for the whole tensor you set no arguments</p>\n",
              "\n",
              "<pre><code>m = t.mean(); print(m) # if you don't set the dim for the whole tensor\n",
              "s = t.std(); print(s) # if you don't set the dim for the whole tensor\n",
              "</code></pre>\n",
              "\n",
              "<p>Then if your shape is 2,2,2 for instance, create tensors for broadcasting subtract and division.</p>\n",
              "\n",
              "<pre><code>ss = torch.empty(2,2,2).fill_(s)\n",
              "print(ss)\n",
              "\n",
              "mm = torch.empty(2,2,2).fill_(m)\n",
              "print(mm)\n",
              "</code></pre>\n",
              "\n",
              "<p>At the moment <code>keepdim</code> is not working as expected when you don't set the <code>dim</code>.</p>\n",
              "\n",
              "<pre><code>m = t.mean(); print(m) # for the whole tensor\n",
              "s = t.std(); print(s) # for the whole tensor\n",
              "\n",
              "m = t.mean(dim=0); print(m) # 0 means columns mean\n",
              "s = t.std(dim=0); print(s) # 0 means columns mean\n",
              "\n",
              "m = t.mean(dim=1); print(m) # 1 means rows mean\n",
              "s = t.std(dim=1); print(s) # 1 means rows mean\n",
              "\n",
              "s = t.mean(keepdim=True);print(s) # will not work\n",
              "m = t.std(keepdim=True);print(m) # will not work\n",
              "</code></pre>\n",
              "\n",
              "<p>If you set a dim as a tuple, then it will return mean for axes, you asked not for the whole.</p>\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_fa864212_0faf_11ec_91bc_0242ac1c0002level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
              "                        <td id=\"T_fa864212_0faf_11ec_91bc_0242ac1c0002row7_col0\" class=\"data row7 col0\" ><p>You can implement your model as below:</p>\n",
              "<pre><code>class Model(nn.Module):\n",
              "       def __init__(self, conv_layers, classifier):\n",
              "           super().__init__()\n",
              "           self.conv_layers = conv_layers\n",
              "           self.classifier = classifier\n",
              "\n",
              "       def forward(self,x):\n",
              "           x = self.conv_layers(x)\n",
              "           return self.classifier(x)\n",
              "</code></pre>\n",
              "<p>When declaring optimizer, only pass the parameters that you want to be updated.</p>\n",
              "<pre><code>       model = Model(conv_layers, classifier)            \n",
              "       optimizer = torch.optim.Adam(model.classifier.parameters(), lr=lr)\n",
              "  \n",
              "</code></pre>\n",
              "<p>Now when you will do</p>\n",
              "<pre><code>       loss.backward()\n",
              "       optimizer.step()\n",
              "       model.zero_grad()\n",
              "</code></pre>\n",
              "<p>only classifier params will be updated.</p>\n",
              "<p>EDIT: After OP's comment, I am adding below for more generic use cases.</p>\n",
              "<p>A more generic scenario</p>\n",
              "<pre><code>   class Model(nn.Module):\n",
              "       def __init__(self, modules):\n",
              "           super().__init__()\n",
              "           # supposing you have multiple modules declared like below. \n",
              "           # You can also keep them as an array or dict too. \n",
              "           # For this see nn.ModuleList or nn.ModuleDict in pytorch \n",
              "           self.module0 = modules[0]\n",
              "           self.module1 = modules[1]\n",
              "           #..... and so on\n",
              "\n",
              "       def forward(self,x):\n",
              "           # implement forward \n",
              "\n",
              "   # model and optimizer declarations\n",
              "   model = Model(modules)  \n",
              "   # assuming we want to update module0 and module1          \n",
              "   optimizer = torch.optim.Adam([\n",
              "       model.module0.parameters(), \n",
              "       model.module1.parameters()\n",
              "   ], lr=lr)\n",
              "   # you can also provide different learning rate for different modules. \n",
              "   # See [documentation][https://pytorch.org/docs/stable/optim.html]\n",
              "   \n",
              "   # when training  \n",
              "   loss.backward()\n",
              "   optimizer.step()\n",
              "   model.zero_grad()\n",
              "   # use model.zero_grad to remove gradient computed for all the modules.\n",
              "   # optimizer.zero_grad only removes gradient for parameters that were passed to it. \n",
              "</code></pre>\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_fa864212_0faf_11ec_91bc_0242ac1c0002level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
              "                        <td id=\"T_fa864212_0faf_11ec_91bc_0242ac1c0002row8_col0\" class=\"data row8 col0\" ><p>Roughly, you need to define a criterion which is the Loss function, for example.</p>\n",
              "<pre><code> import torch.nn as nn\n",
              " criterion = nn.CrossEntropyLoss()\n",
              "</code></pre>\n",
              "<p>Then you need to divide each data entry into two, the image and the corresponding label. Feed the images to your CNN, evaluate with criterion (forward pass) and finally update the CNN (back-propagation).</p>\n",
              "<pre><code># Forward pass\n",
              "outputs = CNN(images)\n",
              "loss = criterion(outputs, labels)\n",
              "\n",
              "# Back pass\n",
              "loss.backward()\n",
              "</code></pre>\n",
              "<p>I suggest you to read some code examples of CNN from GitHub, perhaps a Python Notebook is the best option.</p>\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_fa864212_0faf_11ec_91bc_0242ac1c0002level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
              "                        <td id=\"T_fa864212_0faf_11ec_91bc_0242ac1c0002row9_col0\" class=\"data row9 col0\" ><p>You can set excluded elements to zero: </p>\n",
              "\n",
              "<pre><code>A[range(A.shape[0]),Idx] = 0\n",
              "</code></pre>\n",
              "\n",
              "<p>and sum tensor along rows:</p>\n",
              "\n",
              "<pre><code>b = A.sum(dim = 1,keepdim = True ) # b = torch.tensor([[5],  [9]])\n",
              "</code></pre>\n",
              "</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_fa864212_0faf_11ec_91bc_0242ac1c0002level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
              "                        <td id=\"T_fa864212_0faf_11ec_91bc_0242ac1c0002row10_col0\" class=\"data row10 col0\" ><p>You can use simple einsum:</p>\n",
              "<pre><code>#this gives you 2-D array (M,M)\n",
              "np.einsum('i,ijk-&gt;jk',a,b)\n",
              "</code></pre>\n",
              "<p>output:</p>\n",
              "<pre><code>[[38 44]\n",
              " [50 56]]\n",
              "</code></pre>\n",
              "<p>or another solution:</p>\n",
              "<pre><code>#this gives you 3-D array (1,M,M)\n",
              "a[None,:]@b.swapaxes(0,1)\n",
              "</code></pre>\n",
              "<p>output:</p>\n",
              "<pre><code>[[[38 44]\n",
              "  [50 56]]]\n",
              "</code></pre>\n",
              "</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7facec10e850>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGGKrGgr6u6a"
      },
      "source": [
        "### 4.c - Retriever Model Evaluation\n",
        "We have trained a retrieval model that seems to be working a little better than the traditional word-matching based approach, at least on our running example. Before we use it to actually answer questions, however, we would like to be able to get some quantitative evaluation of the performances of both approaches.\n",
        "\n",
        "For the retriever, we want to favor recall over precision: our first priority is to make sure that all of the information needed to write the answers is present in the support document. If there is unrelated information, the generation model can learn to sort it out. We measure this by computing the proportion of words in the high-scoring answers which are present in the retrieved support document. To focus on important words, we also weigh answer words by their Inverse Document Frequency. This gives us the following IDF-recall scoring function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKbG-TD76ujA"
      },
      "source": [
        ""
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suCtm8mSCI7H"
      },
      "source": [
        "## 5. Generating Answers with a Sequence-to-Sequence Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8rHQ10Zg33h"
      },
      "source": [
        "# ELI5 seq2seq model training\n",
        "###############\n",
        "class ELI5DatasetS2S(Dataset):\n",
        "    def __init__(\n",
        "        self, examples_array,num_rows, make_doc_fun=None, extra_answer_threshold=3, document_cache=None, training=True\n",
        "    ):\n",
        "        self.training = training\n",
        "        self.data = examples_array\n",
        "        self.make_doc_function = make_doc_fun\n",
        "        self.document_cache = {} if document_cache is None else document_cache\n",
        "        self.num_rows = num_rows\n",
        "        assert not (make_doc_fun is None and document_cache is None)\n",
        "        # make index of specific question-answer pairs from multi-answers\n",
        "        if self.training:\n",
        "            self.qa_id_list = [(i, 0) for i in range(self.num_rows)]\n",
        "\n",
        "            #  [\n",
        "            #     (i, j)\n",
        "            #     for i, qa in enumerate(self.data)\n",
        "            #     for j, a in enumerate(qa[\"y\"]))\n",
        "            #     if j == 0 \n",
        "            # ]\n",
        "        else:\n",
        "            self.qa_id_list = [(i, 0) for i in range(self.num_rows)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.qa_id_list)\n",
        "\n",
        "    def make_example(self, idx):\n",
        "        i, j = self.qa_id_list[idx]\n",
        "        example = self.data[i]\n",
        "        question = example[\"x\"] \n",
        "        answer = example[\"y\"]\n",
        "        q_id = example[\"id\"]\n",
        "        if self.make_doc_function is not None:\n",
        "            self.document_cache[q_id] = self.document_cache.get(q_id, self.make_doc_function(example[\"x\"]))\n",
        "        document = self.document_cache[q_id]\n",
        "        in_st = \"question: {} context: {}\".format(\n",
        "            question.lower().strip(), document.lower().strip(),\n",
        "        )\n",
        "        out_st = answer\n",
        "        return (in_st, out_st)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.make_example(idx)\n",
        "\n",
        "\n",
        "def make_qa_s2s_model(model_name=\"facebook/bart-large\", from_file=None, device=\"cuda\"):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
        "    if from_file is not None:\n",
        "        param_dict = torch.load(from_file)  # has model weights, optimizer, and scheduler states\n",
        "        model.load_state_dict(param_dict[\"model\"])\n",
        "    return tokenizer, model\n",
        "\n",
        "\n",
        "def make_qa_s2s_batch(qa_list, tokenizer, max_len=64, max_a_len=128, device=\"cuda\"):\n",
        "    q_ls = [q for q, a in qa_list]\n",
        "    a_ls = [a for q, a in qa_list]\n",
        "    q_toks = tokenizer.batch_encode_plus(q_ls, max_length=max_len, pad_to_max_length=True)\n",
        "    q_ids, q_mask = (\n",
        "        torch.LongTensor(q_toks[\"input_ids\"]).to(device),\n",
        "        torch.LongTensor(q_toks[\"attention_mask\"]).to(device),\n",
        "    )\n",
        "    a_toks = tokenizer.batch_encode_plus(a_ls, max_length=min(max_len, max_a_len), pad_to_max_length=True)\n",
        "    a_ids, a_mask = (\n",
        "        torch.LongTensor(a_toks[\"input_ids\"]).to(device),\n",
        "        torch.LongTensor(a_toks[\"attention_mask\"]).to(device),\n",
        "    )\n",
        "    labels = a_ids[:, 1:].contiguous().clone()\n",
        "    labels[a_mask[:, 1:].contiguous() == 0] = -100\n",
        "    # print(labels)\n",
        "    model_inputs = {\n",
        "        \"input_ids\": q_ids,\n",
        "        \"attention_mask\": q_mask,\n",
        "        \"decoder_input_ids\": a_ids[:, :-1].contiguous(),\n",
        "        \"labels\": labels,\n",
        "    }\n",
        "    # print(\"it'sme\",model_inputs)\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "def train_qa_s2s_epoch(model, dataset, tokenizer, optimizer, scheduler, args, e=0, curriculum=True):\n",
        "    model.train()\n",
        "    # make iterator\n",
        "    if curriculum:\n",
        "        train_sampler = SequentialSampler(dataset)\n",
        "    else:\n",
        "        train_sampler = RandomSampler(dataset)\n",
        "    model_collate_fn = functools.partial(\n",
        "        make_qa_s2s_batch, tokenizer=tokenizer, max_len=args.max_length, device=\"cuda\"\n",
        "    )\n",
        "    data_loader = DataLoader(dataset, batch_size=args.batch_size, sampler=train_sampler, collate_fn=model_collate_fn)\n",
        "    epoch_iterator = tqdm(data_loader, desc=\"Iteration\", disable=True)\n",
        "\n",
        "  \n",
        "    # accumulate loss since last print\n",
        "    loc_steps = 0\n",
        "    loc_loss = 0.0\n",
        "    st_time = time()\n",
        "    for step, batch_inputs in enumerate(epoch_iterator):\n",
        "        # print(type(step))\n",
        "        \n",
        "        pre_loss = model(**batch_inputs)[0]\n",
        "        # print(pre_loss.s(),\"pre sum\")\n",
        "        # print(pre_loss,\"pre loss\")\n",
        "        # print(pre_loss.item(),\"pre shape 0\")\n",
        "        loss = pre_loss.sum() / pre_loss.item()\n",
        "        loss.backward()\n",
        "        # optimizer\n",
        "        if step % args.backward_freq == 0:\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            model.zero_grad()\n",
        "        # some printing within the epoch\n",
        "        loc_loss += loss.item()\n",
        "        loc_steps += 1\n",
        "        if step % args.print_freq == 0 or step == 1:\n",
        "            print(\n",
        "                \"{:2d} {:5d} of {:5d} \\t L: {:.3f} \\t -- {:.3f}\".format(\n",
        "                    e, step, len(dataset) // args.batch_size, loc_loss / loc_steps, time() - st_time,\n",
        "                )\n",
        "            )\n",
        "            loc_loss = 0\n",
        "            loc_steps = 0\n",
        "\n",
        "\n",
        "def eval_qa_s2s_epoch(model, dataset, tokenizer, args):\n",
        "    model.eval()\n",
        "    # make iterator\n",
        "    train_sampler = SequentialSampler(dataset)\n",
        "    model_collate_fn = functools.partial(\n",
        "        make_qa_s2s_batch, tokenizer=tokenizer, max_len=args.max_length, device=\"cuda\"\n",
        "    )\n",
        "    data_loader = DataLoader(dataset, batch_size=args.batch_size, sampler=train_sampler, collate_fn=model_collate_fn)\n",
        "    epoch_iterator = tqdm(data_loader, desc=\"Iteration\", disable=True)\n",
        "    # accumulate loss since last print\n",
        "    loc_steps = 0\n",
        "    loc_loss = 0.0\n",
        "    st_time = time()\n",
        "    with torch.no_grad():\n",
        "        for step, batch_inputs in enumerate(epoch_iterator):\n",
        "            pre_loss = model(**batch_inputs)[0]\n",
        "            # print(pre_loss)\n",
        "            # print(pre_loss.size())\n",
        "            loss = pre_loss.sum() / pre_loss.item()\n",
        "            loc_loss += loss.item()\n",
        "            loc_steps += 1\n",
        "            if step % args.print_freq == 0:\n",
        "                print(\n",
        "                    \"{:5d} of {:5d} \\t L: {:.3f} \\t -- {:.3f}\".format(\n",
        "                        step, len(dataset) // args.batch_size, loc_loss / loc_steps, time() - st_time,\n",
        "                    )\n",
        "                )\n",
        "    print(\"Total \\t L: {:.3f} \\t -- {:.3f}\".format(loc_loss / loc_steps, time() - st_time,))\n",
        "\n",
        "\n",
        "def train_qa_s2s(qa_s2s_model, qa_s2s_tokenizer, s2s_train_dset, s2s_valid_dset, s2s_args):\n",
        "    s2s_optimizer = AdamW(qa_s2s_model.parameters(), lr=s2s_args.learning_rate, eps=1e-8)\n",
        "    s2s_scheduler = get_linear_schedule_with_warmup(\n",
        "        s2s_optimizer,\n",
        "        num_warmup_steps=400,\n",
        "        num_training_steps=(s2s_args.num_epochs + 1) * math.ceil(len(s2s_train_dset) / s2s_args.batch_size),\n",
        "    )\n",
        "    for e in range(s2s_args.num_epochs):\n",
        "        # print((e == 0))\n",
        "\n",
        "        train_qa_s2s_epoch(\n",
        "            qa_s2s_model,\n",
        "            s2s_train_dset,\n",
        "            qa_s2s_tokenizer,\n",
        "            s2s_optimizer,\n",
        "            s2s_scheduler,\n",
        "            s2s_args,\n",
        "            e,\n",
        "            curriculum=True,\n",
        "        )\n",
        "        m_save_dict = {\n",
        "            \"model\": qa_s2s_model.state_dict(),\n",
        "            \"optimizer\": s2s_optimizer.state_dict(),\n",
        "            \"scheduler\": s2s_scheduler.state_dict(),\n",
        "        }\n",
        "        print(\"Saving model {}\".format(s2s_args.model_save_name))\n",
        "        eval_qa_s2s_epoch(qa_s2s_model, s2s_valid_dset, qa_s2s_tokenizer, s2s_args)\n",
        "        torch.save(m_save_dict, \"{}_{}.pth\".format(s2s_args.model_save_name, e))\n",
        "\n",
        "\n",
        "# generate answer from input \"question: ... context: <p> ...\"\n",
        "def qa_s2s_generate(\n",
        "    question_doc,\n",
        "    qa_s2s_model,\n",
        "    qa_s2s_tokenizer,\n",
        "    num_answers=1,\n",
        "    num_beams=None,\n",
        "    min_len=2,\n",
        "    max_len=64,\n",
        "    do_sample=False,\n",
        "    temp=1.0,\n",
        "    top_p=None,\n",
        "    top_k=None,\n",
        "    max_input_length=128,\n",
        "    device=\"cuda\",\n",
        "):\n",
        "    model_inputs = make_qa_s2s_batch([(question_doc, \"A\")], qa_s2s_tokenizer, max_input_length, device=device,)\n",
        "    n_beams = num_answers if num_beams is None else max(num_beams, num_answers)\n",
        "    generated_ids = qa_s2s_model.generate(\n",
        "        input_ids=model_inputs[\"input_ids\"],\n",
        "        attention_mask=model_inputs[\"attention_mask\"],\n",
        "        min_length=min_len,\n",
        "        max_length=max_len,\n",
        "        do_sample=do_sample,\n",
        "        early_stopping=True,\n",
        "        num_beams=1 if do_sample else n_beams,\n",
        "        temperature=temp,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        eos_token_id=qa_s2s_tokenizer.eos_token_id,\n",
        "        no_repeat_ngram_size=3,\n",
        "        num_return_sequences=num_answers,\n",
        "        decoder_start_token_id=qa_s2s_tokenizer.bos_token_id,\n",
        "    )\n",
        "    return [qa_s2s_tokenizer.decode(ans_ids, skip_special_tokens=True).strip() for ans_ids in generated_ids]\n"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-Sno_bWeWkL"
      },
      "source": [
        "n_ret = 2"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5Fhed0htCIL5",
        "outputId": "d6bfba43-1424-4803-fbb2-9706fff8cc1e"
      },
      "source": [
        "# pre-computing support documents\n",
        "eli5_train_docs = []\n",
        "for example in train:\n",
        "    support_doc, dense_res_list = query_qa_dense_index(\n",
        "        example['x'], qar_model, qar_tokenizer,passage_snippets, wiki40b_gpu_index, n_results=n_ret\n",
        "    )\n",
        "    eli5_train_docs += [(example['id'], support_doc, dense_res_list)]\n",
        "\n",
        "eli5_valid_docs = []\n",
        "for example in test:\n",
        "    support_doc, dense_res_list = query_qa_dense_index(\n",
        "        example['x'], qar_model, qar_tokenizer, passage_snippets, wiki40b_gpu_index, n_results=n_ret\n",
        "    )\n",
        "    eli5_valid_docs += [(example['id'], support_doc, dense_res_list)]\n",
        "\n",
        "# training loop proper\n",
        "class ArgumentsS2S():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 2\n",
        "        self.backward_freq = 16\n",
        "        self.max_length = 64\n",
        "        self.print_freq = 100\n",
        "        self.model_save_name = \"seq2seq_models/eli5_bart_model\"\n",
        "        self.learning_rate = 2e-4\n",
        "        self.num_epochs =1\n",
        "\n",
        "s2s_args = ArgumentsS2S()\n",
        "\n",
        "# eli5_train_docs = json.load(open('precomputed/eli5_train_precomputed_dense_docs.json'))\n",
        "# eli5_valid_docs = json.load(open('precomputed/eli5_valid_precomputed_dense_docs.json'))\n",
        "s2s_train_dset = ELI5DatasetS2S(train,num_rows =len(train), document_cache=dict([(k, d) for k, d, src_ls in eli5_train_docs]))\n",
        "s2s_valid_dset = ELI5DatasetS2S(test,num_rows =len(test), document_cache=dict([(k, d) for k, d, src_ls in eli5_valid_docs]), training=False)\n",
        "\n",
        "qa_s2s_tokenizer, pre_model = make_qa_s2s_model(\n",
        "    model_name=\"facebook/bart-large\",\n",
        "    from_file=None,\n",
        "    device=\"cuda\"\n",
        ")\n",
        "qa_s2s_model = torch.nn.DataParallel(pre_model)\n",
        "\n",
        "train_qa_s2s(qa_s2s_model, qa_s2s_tokenizer, s2s_train_dset, s2s_valid_dset, s2s_args)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 0     0 of 12565 \t L: 1.000 \t -- 0.314\n",
            " 0     1 of 12565 \t L: 1.000 \t -- 0.448\n",
            " 0   100 of 12565 \t L: 1.000 \t -- 15.146\n",
            " 0   200 of 12565 \t L: 1.000 \t -- 30.247\n",
            " 0   300 of 12565 \t L: 1.000 \t -- 45.351\n",
            " 0   400 of 12565 \t L: 1.000 \t -- 60.440\n",
            " 0   500 of 12565 \t L: 1.000 \t -- 75.337\n",
            " 0   600 of 12565 \t L: 1.000 \t -- 90.279\n",
            " 0   700 of 12565 \t L: 1.000 \t -- 105.256\n",
            " 0   800 of 12565 \t L: 1.000 \t -- 120.367\n",
            " 0   900 of 12565 \t L: 1.000 \t -- 135.351\n",
            " 0  1000 of 12565 \t L: 1.000 \t -- 150.283\n",
            " 0  1100 of 12565 \t L: 1.000 \t -- 165.243\n",
            " 0  1200 of 12565 \t L: 1.000 \t -- 180.293\n",
            " 0  1300 of 12565 \t L: 1.000 \t -- 195.263\n",
            " 0  1400 of 12565 \t L: 1.000 \t -- 210.200\n",
            " 0  1500 of 12565 \t L: 1.000 \t -- 225.161\n",
            " 0  1600 of 12565 \t L: 1.000 \t -- 240.293\n",
            " 0  1700 of 12565 \t L: 1.000 \t -- 255.259\n",
            " 0  1800 of 12565 \t L: 1.000 \t -- 270.229\n",
            " 0  1900 of 12565 \t L: 1.000 \t -- 285.168\n",
            " 0  2000 of 12565 \t L: 1.000 \t -- 300.262\n",
            " 0  2100 of 12565 \t L: 1.000 \t -- 315.198\n",
            " 0  2200 of 12565 \t L: 1.000 \t -- 330.177\n",
            " 0  2300 of 12565 \t L: 1.000 \t -- 345.088\n",
            " 0  2400 of 12565 \t L: 1.000 \t -- 360.164\n",
            " 0  2500 of 12565 \t L: 1.000 \t -- 375.118\n",
            " 0  2600 of 12565 \t L: 1.000 \t -- 390.040\n",
            " 0  2700 of 12565 \t L: 1.000 \t -- 404.966\n",
            " 0  2800 of 12565 \t L: 1.000 \t -- 420.038\n",
            " 0  2900 of 12565 \t L: 1.000 \t -- 434.994\n",
            " 0  3000 of 12565 \t L: 1.000 \t -- 449.947\n",
            " 0  3100 of 12565 \t L: 1.000 \t -- 464.900\n",
            " 0  3200 of 12565 \t L: 1.000 \t -- 479.957\n",
            " 0  3300 of 12565 \t L: 1.000 \t -- 494.917\n",
            " 0  3400 of 12565 \t L: 1.000 \t -- 509.882\n",
            " 0  3500 of 12565 \t L: 1.000 \t -- 524.812\n",
            " 0  3600 of 12565 \t L: 1.000 \t -- 539.933\n",
            " 0  3700 of 12565 \t L: 1.000 \t -- 554.864\n",
            " 0  3800 of 12565 \t L: 1.000 \t -- 569.795\n",
            " 0  3900 of 12565 \t L: 1.000 \t -- 584.750\n",
            " 0  4000 of 12565 \t L: 1.000 \t -- 599.801\n",
            " 0  4100 of 12565 \t L: 1.000 \t -- 614.730\n",
            " 0  4200 of 12565 \t L: 1.000 \t -- 629.667\n",
            " 0  4300 of 12565 \t L: 1.000 \t -- 644.632\n",
            " 0  4400 of 12565 \t L: 1.000 \t -- 659.761\n",
            " 0  4500 of 12565 \t L: 1.000 \t -- 674.667\n",
            " 0  4600 of 12565 \t L: 1.000 \t -- 689.598\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-b2c0cc5acc94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mqa_s2s_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mtrain_qa_s2s\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqa_s2s_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqa_s2s_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2s_train_dset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2s_valid_dset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2s_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-81-b6c35b4a77f8>\u001b[0m in \u001b[0;36mtrain_qa_s2s\u001b[0;34m(qa_s2s_model, qa_s2s_tokenizer, s2s_train_dset, s2s_valid_dset, s2s_args)\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0ms2s_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0mcurriculum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         )\n\u001b[1;32m    178\u001b[0m         m_save_dict = {\n",
            "\u001b[0;32m<ipython-input-81-b6c35b4a77f8>\u001b[0m in \u001b[0;36mtrain_qa_s2s_epoch\u001b[0;34m(model, dataset, tokenizer, optimizer, scheduler, args, e, curriculum)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# some printing within the epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mloc_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0mloc_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}